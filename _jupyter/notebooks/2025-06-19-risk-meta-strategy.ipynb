{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2ea2829a-24a9-4d3f-bd07-0d9ad73f2d16",
   "metadata": {},
   "source": [
    "---\n",
    "layout: article\n",
    "title: The Meta-Strategy of Eliminating Risk\n",
    "custom_css: article.css\n",
    "include_mathjax: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed8bddb-72b9-472e-8403-8b348b427783",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When outcomes are uncertain, simply choosing the option with the highest expected reward is rarely enough. Real-world decisions must account for **risk** — the variance, volatility, or unpredictability that surrounds those expected returns. Once risk enters the equation, everything changes.\n",
    "\n",
    "This article explores a striking mathematical fact:\n",
    "\n",
    "> Any utility function that penalizes variance will naturally converge on a strategy that is not a single option, but a portfolio — a distribution over multiple options.\n",
    "\n",
    "In other words, **risk-aware optimization does not lead to decisions; it leads to distributions**. The optimal solution is no longer a point on a line, but a **vector in strategy space** — a blend of actions in precise proportions that reduces uncertainty while preserving reward.\n",
    "\n",
    "This shift reflects something deeper: a structural rule about the nature of optimal solutions — what we call a [meta-strategy](https://diogenesanalytics.com/blog/2025/05/31/hierarchy-as-meta-strategy). A meta-strategy is not a particular choice, but a principle that governs how choices must be structured. In this case, it tells us that optimal strategies must lie *within* the space of distributions — not at the edges. Rather than selecting a single action, the system must commit to a weighted mixture of them. It is a higher-level pattern: utility functions that penalize variance force solutions to become **distributed**.\n",
    "\n",
    "This is not just theoretical elegance. The same pattern emerges across domains:\n",
    "\n",
    "* In **finance**, diversified portfolios reduce risk without sacrificing expected return.\n",
    "* In **evolution**, organisms hedge reproductive success across environmental niches.\n",
    "* In **physics**, repeated scans suppress noise to uncover weak signals.\n",
    "* In **machine learning**, ensembles stabilize predictions by averaging models.\n",
    "\n",
    "In each case, blending options outperforms betting on just one. It is not merely safer — it is mathematically optimal.\n",
    "\n",
    "As we will see, this meta-strategy transforms how we think about decision-making. The best strategy is not a choice — it is a **design**. A construction that leverages structure, suppresses variance, and embraces distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a030f54-c02a-46f2-b02d-dd96fb54540f",
   "metadata": {},
   "source": [
    "## Utility Functions and the Cost of Variance\n",
    "In many decision-making scenarios, selecting the option with the highest expected return is insufficient. Real-world decisions often involve uncertainty, and strategies must be evaluated not just by their average payoff but also by their risk — the variability in their outcomes. To capture this, we consider a broad class of utility functions that explicitly penalize variance.\n",
    "\n",
    "A common form is:\n",
    "\n",
    "$$\n",
    "U(x) = \\mathbb{E}[R(x)] - \\lambda \\cdot \\text{Var}(R(x))\n",
    "$$\n",
    "\n",
    "where $R(x)$ is the random return from strategy $x$, and $\\lambda > 0$ is a risk-aversion parameter. This form is foundational in finance, especially in mean-variance portfolio theory, and appears in decision theory and control systems as well.\n",
    "\n",
    "An alternative formulation scales expected reward by volatility:\n",
    "\n",
    "$$\n",
    "U(x) = \\frac{\\mathbb{E}[R(x)]}{\\sigma[R(x)]^\\alpha}\n",
    "$$\n",
    "\n",
    "This is closely related to the Sharpe ratio when $\\alpha = 1$, and emphasizes the trade-off between reward and consistency. Both formulations capture a core idea: strategies with more predictable outcomes are preferred, even if their expected value is lower.\n",
    "\n",
    "To see the implications of this, suppose we are choosing between two independent options:\n",
    "\n",
    "* **Option A**: $\\mathbb{E}[R_A] = 10$, $\\sigma_A = 5$\n",
    "* **Option B**: $\\mathbb{E}[R_B] = 8$, $\\sigma_B = 1$\n",
    "\n",
    "With a risk penalty of $\\lambda = 0.5$, we compute their utilities:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "U(A) = 10 - 0.5 \\cdot (5)^2 = 10 - 12.5 = -2.5 \\\\\n",
    "U(B) = 8 - 0.5 \\cdot (1)^2 = 8 - 0.5 = 7.5\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So despite having a lower expected return, Option B is preferred because of its much lower variance. But more interestingly: can we do even better by blending them?\n",
    "\n",
    "Let $\\pi = (\\pi_A, \\pi_B)$ be a mixed strategy where $\\pi_A + \\pi_B = 1$. The expected return of this portfolio is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R_\\pi] = \\pi_A \\cdot \\mathbb{E}[R_A] + \\pi_B \\cdot \\mathbb{E}[R_B]\n",
    "$$\n",
    "\n",
    "Assuming the returns are independent (so covariance is zero), the variance of the portfolio is:\n",
    "\n",
    "$$\n",
    "\\text{Var}(R_\\pi) = \\pi_A^2 \\cdot \\sigma_A^2 + \\pi_B^2 \\cdot \\sigma_B^2\n",
    "$$\n",
    "\n",
    "The utility becomes:\n",
    "\n",
    "$$\n",
    "U(\\pi) = \\mathbb{E}[R_\\pi] - \\lambda \\cdot \\text{Var}(R_\\pi)\n",
    "$$\n",
    "\n",
    "By adjusting the weights $\\pi_A$ and $\\pi_B$, we can find a portfolio that achieves a better utility than either A or B alone. This highlights a fundamental point: **the optimal strategy is not a choice, but a vector — a distribution over multiple options**. This is where scalar thinking breaks down and the logic of diversification emerges.\n",
    "\n",
    "Let us simulate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bbb3d-e15a-4441-9964-588992045551",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set initial figure counter to 1\n",
    "fig_count = 1\n",
    "\n",
    "# set the style to a dark theme\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# match website background\n",
    "plt.rcParams[\"figure.facecolor\"] = \"#181818\"\n",
    "plt.rcParams[\"axes.facecolor\"] = \"#181818\"\n",
    "plt.rcParams[\"axes.edgecolor\"] = \"#181818\"\n",
    "\n",
    "# Parameters for Option A and B\n",
    "mu_A, sigma_A = 10, 5\n",
    "mu_B, sigma_B = 8, 1\n",
    "lambda_risk = 0.5\n",
    "\n",
    "# Range of portfolio weights from 0% A to 100% A\n",
    "weights = np.linspace(0, 1, 100)\n",
    "expected_returns = weights * mu_A + (1 - weights) * mu_B\n",
    "variances = (weights ** 2) * (sigma_A ** 2) + ((1 - weights) ** 2) * (sigma_B ** 2)\n",
    "utilities = expected_returns - lambda_risk * variances\n",
    "\n",
    "# Find the maximum utility point\n",
    "optimal_index = np.argmax(utilities)\n",
    "optimal_weight = weights[optimal_index]\n",
    "optimal_utility = utilities[optimal_index]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(weights, utilities, label=\"Utility\", color=plt.cm.viridis(0.70))\n",
    "plt.axvline(optimal_weight, color='red', linestyle='--', label=f'Optimal Blend: {optimal_weight:.2f}')\n",
    "\n",
    "# labels\n",
    "plt.xlabel(\"Weight on Option A (1 - B)\")\n",
    "plt.ylabel(\"Utility\")\n",
    "\n",
    "# set title\n",
    "plt.suptitle(\n",
    "    f\"Figure {fig_count}. Utility of Blended Strategies vs. Weight on Option A\", y=0.0001, fontsize=10\n",
    ")\n",
    "\n",
    "# increment fig count\n",
    "fig_count += 1\n",
    "\n",
    "# display\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show();\n",
    "\n",
    "# increment fig count\n",
    "fig_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcd529-1a26-4728-b906-0882dd15eeb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"Maximum utility achieved at weight on Option A = {optimal_weight:.2f}, utility = {optimal_utility:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902efa23-a879-4314-a248-555f83ef6fc6",
   "metadata": {},
   "source": [
    "This simulation confirms the theory: the highest utility occurs at a **blend** of A and B. \n",
    "Neither pure strategy is optimal. By using a vector of weights — a portfolio — we achieve a better balance between\n",
    "reward and risk.\n",
    "\n",
    "This is not an artifact of our toy example; it is a general principle. Any utility function that penalizes variance will,\n",
    "by necessity, favor distributions over single choices. It is in the geometry of the utility function itself. To reduce \n",
    "variance below the limit of any single option, we must move to a **space of vectors** — a strategy that spans multiple possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c9fa2-f955-43c9-86ce-e58b99c1be62",
   "metadata": {},
   "source": [
    "## The Law of Large Numbers and Variance Collapse\n",
    "So far, we have seen that utility functions which penalize variance push us away from single options and toward blended strategies — portfolios. But what exactly gives these portfolios their power? Why does blending reduce risk, mathematically?\n",
    "\n",
    "The answer lies in one of the foundational theorems of probability theory: the **Law of Large Numbers**.\n",
    "\n",
    "The LLN tells us that when we take many independent random samples from the same distribution, their average converges to the expected value. But more than that — the **variance of the average decreases** as the number of samples increases. Specifically, for i.i.d. variables $X_1, X_2, \\dots, X_n$ with mean $\\mu$ and variance $\\sigma^2$, the average:\n",
    "\n",
    "$$\n",
    "\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$$\n",
    "\n",
    "has variance:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "This is profound: averaging doesn’t just stabilize outcomes — it **collapses variance**. And this collapse happens quickly: doubling the number of samples cuts the variance in half.\n",
    "\n",
    "In the context of strategy, this means that by distributing effort across many independent options — even if each option is noisy — the **aggregate outcome becomes more predictable**. Risk, as measured by variance, diminishes with each added component.\n",
    "\n",
    "But there's a hidden catch: this only works if the options are not perfectly correlated. If the returns of the options move together (i.e. high covariance), diversification does little. On the other hand, if the options are independent — or better yet, negatively correlated — then a portfolio strategy can rapidly drive variance toward zero, even if each component is risky on its own.\n",
    "\n",
    "This explains why diversification is so powerful, and why the vector strategy emerges naturally. When utility penalizes variance, the optimization algorithm is doing nothing more than applying the Law of Large Numbers — constructing combinations whose average is stable and whose fluctuations cancel out.\n",
    "\n",
    "It’s signal averaging, mathematically formalized.\n",
    "\n",
    "In finance, this is why uncorrelated assets are so prized: they create portfolios where individual volatilities offset one another. In ensemble machine learning, this is why random forests and bagged models outperform single estimators — variance shrinks as outputs are averaged. And in NMR, it’s why repeated scans of the same faint signal reveal structure where none was visible before: the signal remains, the noise washes out.\n",
    "\n",
    "In each case, the LLN is the invisible engine reducing risk. And utility functions that penalize variance are simply listening to it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
